# The paths section describes paths for data and environments
[paths]

# A shared root directory where polaris data can be found
database_root = /lustre/orion/cli115/world-shared/polaris

# the path to the base conda environment where polaris environments have
# been created
polaris_envs = /ccs/proj/cli115/software/polaris/frontier/conda/base


# Options related to deploying a polaris conda and spack environments
[deploy]

# the compiler set to use for system libraries and MPAS builds
compiler = craygnu

# the compiler to use to build software (e.g. ESMF and MOAB) with spack
software_compiler = craygnu

# the system MPI library to use for craygnu compiler
mpi_craygnu = mpich

# the system MPI library to use for craygnu-mphipcc compiler
mpi_craygnu_mphipcc = mpich

# the system MPI library to use for craycray compiler
mpi_craycray = mpich

# the system MPI library to use for craycray-mphipcc compiler
mpi_craycray_mphipcc = mpich

# the system MPI library to use for crayamd compiler
mpi_crayamd = mpich

# the system MPI library to use for crayamd-mphipcc compiler
mpi_crayamd_mphipcc = mpich

# the base path for spack environments used by polaris
spack = /ccs/proj/cli115/software/polaris/frontier/spack

# whether to use the same modules for hdf5, netcdf-c, netcdf-fortran and
# pnetcdf as E3SM (spack modules are used otherwise)
use_e3sm_hdf5_netcdf = True

# The parallel section describes options related to running jobs in parallel.
# Most options in this section come from mache so here we just add or override
# some defaults
[parallel]

# allocatable cores per node on the machine
cores_per_node = 56

# threads per core (set to 1 because hyperthreading requires extra sbatch
# flag --threads-per-core that polaris doesn't yet support)
threads_per_core = 1
