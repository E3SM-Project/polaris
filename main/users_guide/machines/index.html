

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Machines</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=b208ceb8"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Anvil" href="anvil.html" />
    <link rel="prev" title="single column" href="../seaice/tasks/single_column.html" />
    <link href="../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Polaris
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User's guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quick_start.html">Quick Start for Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tasks.html">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_files.html">Config Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="../suites.html">Suites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ocean/index.html">Ocean component</a></li>
<li class="toctree-l1"><a class="reference internal" href="../seaice/index.html">Sea ice component</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Machines</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#config-options">config options</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slurm-job-queueing">Slurm job queueing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-machines">Supported Machines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="anvil.html">Anvil</a></li>
<li class="toctree-l3"><a class="reference internal" href="chicoma.html">Chicoma</a></li>
<li class="toctree-l3"><a class="reference internal" href="chrysalis.html">Chrysalis</a></li>
<li class="toctree-l3"><a class="reference internal" href="compy.html">CompyMcNodeFace</a></li>
<li class="toctree-l3"><a class="reference internal" href="frontier.html">Frontier</a></li>
<li class="toctree-l3"><a class="reference internal" href="perlmutter.html">Perlmutter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#other-machines">Other Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mpich">MPICH</a></li>
<li class="toctree-l2"><a class="reference internal" href="#openmpi">OpenMPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#no-mpi-from-conda-forge">No MPI from conda-forge</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer's guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/quick_start.html">Quick Start for Developers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/command_line.html">Command-line interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/organization/index.html">Organization of Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/ocean/index.html">Ocean component</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/seaice/index.html">SeaIce component</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/framework/index.html">Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/machines/index.html">Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/docs.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/building_docs.html">Building the Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/deploying_spack.html">Deploying a new spack environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers_guide/api.html">API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design_docs/index.html">Design Documents</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/dev_add_test_group.html">Developer Tutorial: Adding a new test group</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Polaris</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Machines</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/users_guide/machines/index.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="machines">
<span id="id1"></span><h1>Machines<a class="headerlink" href="#machines" title="Link to this heading"></a></h1>
<p>Polaris  attempts to be aware of the capabilities of the machine it is running
on.  This is a particular advantage for so-called “supported” machines with a
config file defined for them in the <code class="docutils literal notranslate"><span class="pre">polaris</span></code> package.  But even for “unknown”
machines, it is not difficult to set a few config options in your user config
file to describe your machine.  Then, polaris can use this data to make sure
tasks are configured in a way that is appropriate for your machine.</p>
<section id="config-options">
<h2>config options<a class="headerlink" href="#config-options" title="Link to this heading"></a></h2>
<p>The config options typically defined for a machine are:</p>
<div class="highlight-cfg notranslate"><div class="highlight"><pre><span></span><span class="c1"># The paths section describes paths that are used within the ocean core test</span>
<span class="c1"># cases.</span>
<span class="k">[paths]</span>

<span class="c1"># A shared root directory where MPAS standalone data can be found</span>
<span class="na">database_root</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">/lcrc/group/e3sm/public_html/mpas_standalonedata</span>

<span class="c1"># the path to the base conda environment where polaris environments have</span>
<span class="c1"># been created</span>
<span class="na">polaris_envs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">/lcrc/soft/climate/polaris/chrysalis/base</span>


<span class="c1"># Options related to deploying a polaris conda environment on supported</span>
<span class="c1"># machines</span>
<span class="k">[deploy]</span>

<span class="c1"># the compiler set to use for system libraries and MPAS builds</span>
<span class="na">compiler</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">intel</span>

<span class="c1"># the system MPI library to use for intel compiler</span>
<span class="na">mpi_intel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">openmpi</span>

<span class="c1"># the system MPI library to use for gnu compiler</span>
<span class="na">mpi_gnu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">openmpi</span>

<span class="c1"># the base path for spack environments used by polaris</span>
<span class="na">spack</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">/lcrc/soft/climate/polaris/chrysalis/spack</span>

<span class="c1"># whether to use the same modules for hdf5, netcdf-c, netcdf-fortran and</span>
<span class="c1"># pnetcdf as E3SM (spack modules are used otherwise)</span>
<span class="na">use_e3sm_hdf5_netcdf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">True</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">paths</span></code> section provides local paths to the root of the “databases”
(local caches) of data files for each MPAS core.  These are generally in a
shared location for the project to save space.  Similarly, <code class="docutils literal notranslate"><span class="pre">polaris_envs</span></code>
is a location where shared conda environments will be created for polaris
releases for users to share.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">deploy</span></code> section is used to help polaris create development and
release conda environments and activation scripts.  It says which compiler set
is the default, which MPI library is the default for each supported compiler,
and where libraries built with system MPI will be placed.</p>
<p>Some config options come from a package, <a class="reference external" href="https://github.com/E3SM-Project/mache/">mache</a>
that is a dependency of polaris.  Mache is designed to detect and
provide a machine-specific configuration for E3SM supported machines.  Typical
config options provided by mache that are relevant to polaris are:</p>
<div class="highlight-cfg notranslate"><div class="highlight"><pre><span></span><span class="c1"># The parallel section describes options related to running jobs in parallel</span>
<span class="k">[parallel]</span>

<span class="c1"># parallel system of execution: slurm, cobalt or single_node</span>
<span class="na">system</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">slurm</span>

<span class="c1"># whether to use mpirun or srun to run a task</span>
<span class="na">parallel_executable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">srun</span>

<span class="c1"># cores per node on the machine</span>
<span class="na">cores_per_node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">36</span>

<span class="c1"># account for running diagnostics jobs</span>
<span class="na">account</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">e3sm</span>

<span class="c1"># quality of service (default is the first)</span>
<span class="na">qos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">regular, interactive</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">parallel</span></code> section defined properties of the machine, to do with parallel
runs. Currently, machine files are defined for high-performance computing (HPC)
machines with multiple nodes.  These machines all use <a class="reference internal" href="#slurm"><span class="std std-ref">Slurm job queueing</span></a> to submit
parallel jobs.  They also all use the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command to run individual
tasks within a job.  The number of <code class="docutils literal notranslate"><span class="pre">cores_per_node</span></code> vary between machines,
as does the account that typical polaris users will have access to on the
machine.</p>
</section>
<section id="slurm-job-queueing">
<span id="slurm"></span><h2>Slurm job queueing<a class="headerlink" href="#slurm-job-queueing" title="Link to this heading"></a></h2>
<p>Most HPC systems now use the
<a class="reference external" href="https://slurm.schedmd.com/documentation.html">slurm workload manager</a>.
Here are some basic commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>salloc<span class="w"> </span>-N<span class="w"> </span><span class="m">1</span><span class="w"> </span>-t<span class="w"> </span><span class="m">2</span>:0:0<span class="w"> </span><span class="c1"># interactive job (see machine specific versions below)</span>
sbatch<span class="w"> </span>script<span class="w"> </span><span class="c1"># submit a script</span>
squeue<span class="w"> </span><span class="c1"># show all jobs</span>
squeue<span class="w"> </span>-u<span class="w"> </span>&lt;my_username&gt;<span class="w"> </span><span class="c1"># show only your jobs</span>
scancel<span class="w"> </span>jobID<span class="w"> </span><span class="c1"># cancel a job</span>
</pre></div>
</div>
</section>
<section id="supported-machines">
<span id="id2"></span><h2>Supported Machines<a class="headerlink" href="#supported-machines" title="Link to this heading"></a></h2>
<p>On each supported machine, users will be able to source a script to activate
the appropriate polaris environment and compilers.  Most machines support 2
compilers, each with one or more variants of MPI and the required NetCDF,
pNetCDF and SCORPIO libraries.  These scripts will first load the conda
environment for polaris, then it will load modules and set environment
variables that will allow you to build and run the MPAS model.</p>
<p>A table with the full list of supported machines, compilers, MPI variants,
and MPAS-model build commands is found in <a class="reference internal" href="../../developers_guide/machines/index.html#dev-supported-machines"><span class="std std-ref">Supported Machines</span></a> in
the Developer’s Guide.  In the links below, we list only the commands needed
to use the default MPI variant for each compiler on each machine.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="anvil.html">Anvil</a></li>
<li class="toctree-l1"><a class="reference internal" href="chicoma.html">Chicoma</a></li>
<li class="toctree-l1"><a class="reference internal" href="chrysalis.html">Chrysalis</a></li>
<li class="toctree-l1"><a class="reference internal" href="compy.html">CompyMcNodeFace</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontier.html">Frontier</a></li>
<li class="toctree-l1"><a class="reference internal" href="perlmutter.html">Perlmutter</a></li>
</ul>
</div>
</section>
<section id="other-machines">
<span id="id3"></span><h2>Other Machines<a class="headerlink" href="#other-machines" title="Link to this heading"></a></h2>
<p>If you are working on an “unknown” machine, you will need to define some of
the config options that would normally be in a machine’s config file yourself
in your user config file:</p>
<div class="highlight-cfg notranslate"><div class="highlight"><pre><span></span><span class="c1"># This file contains some common config options you might want to set</span>

<span class="c1"># The paths section describes paths to databases and shared polaris environments</span>
<span class="k">[paths]</span>

<span class="c1"># A root directory where MPAS standalone data can be found</span>
<span class="na">database_root</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">/home/xylar/data/mpas/mpas_standalonedata</span>

<span class="c1"># The parallel section describes options related to running tasks in parallel</span>
<span class="k">[parallel]</span>

<span class="c1"># parallel system of execution: slurm or single_node</span>
<span class="na">system</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">single_node</span>

<span class="c1"># whether to use mpirun or srun to run the model</span>
<span class="na">parallel_executable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">mpirun -host localhost</span>

<span class="c1"># cores per node on the machine, detected automatically by default</span>
<span class="c1"># cores_per_node = 4</span>
</pre></div>
</div>
<p>The paths for the MPAS core “databases” can be any emtpy path to begin with.
If the path doesn’t exist, polaris will create it.</p>
<p>If you’re not working on an HPC machine, you will probably not have multiple
nodes or <a class="reference internal" href="#slurm"><span class="std std-ref">Slurm job queueing</span></a>.  You will probably install
<a class="reference external" href="https://www.mpich.org/">MPICH</a> or <a class="reference external" href="https://www.open-mpi.org/">OpenMPI</a>,
probably via a
<a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/index.html">conda environment</a>.
In this case, the <code class="docutils literal notranslate"><span class="pre">parallel_executable</span></code> is <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>.</p>
<p>To install the <code class="docutils literal notranslate"><span class="pre">polaris</span></code> package into a conda environment, you will first
need to install <a class="reference external" href="https://github.com/conda-forge/miniforge#miniforge3">Miniforge3</a>
(if it is not already installed).  Then, you will run one of the following
three commands, depending on how you would like to handle MPI support in the
conda packages.</p>
</section>
<section id="mpich">
<h2>MPICH<a class="headerlink" href="#mpich" title="Link to this heading"></a></h2>
<p>To create a conda environment called “polaris” with MPI from the <code class="docutils literal notranslate"><span class="pre">mpich</span></code>
package, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>polaris<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-c<span class="w"> </span>e3sm/label/polaris<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="w"> </span><span class="s2">&quot;polaris=*=mpi_mpich*&quot;</span>
</pre></div>
</div>
<p>This is the recommended default for single-node Linux and OSX machines.</p>
</section>
<section id="openmpi">
<h2>OpenMPI<a class="headerlink" href="#openmpi" title="Link to this heading"></a></h2>
<p>To create a conda environment called “polaris” with MPI from the <code class="docutils literal notranslate"><span class="pre">openmpi</span></code>
package, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>polaris<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-c<span class="w"> </span>e3sm/label/polaris<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="w"> </span><span class="s2">&quot;polaris=*=mpi_openmpi*&quot;</span>
</pre></div>
</div>
</section>
<section id="no-mpi-from-conda-forge">
<h2>No MPI from conda-forge<a class="headerlink" href="#no-mpi-from-conda-forge" title="Link to this heading"></a></h2>
<p>To create a conda environment called “polaris” without any MPI package from
conda-forge, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>polaris<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-c<span class="w"> </span>e3sm/label/polaris<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="w"> </span><span class="s2">&quot;polaris=*=nompi*&quot;</span>
</pre></div>
</div>
<p>This would be the starting point for working with polaris on an unknown
HPC machine.  From there, you would also need to load modules and set
environment variables so that MPAS components can be built with system NetCDF,
pNetCDF and SCORPIO. This will likely require working with an MPAS developer.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../seaice/tasks/single_column.html" class="btn btn-neutral float-left" title="single column" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="anvil.html" class="btn btn-neutral float-right" title="Anvil" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Energy Exascale Earth System Model Project.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../../0.5.0/index.html">0.5.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="index.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>